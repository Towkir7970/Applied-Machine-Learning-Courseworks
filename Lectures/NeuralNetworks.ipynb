{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Artificial Neural Networks (ANN)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Neural networks\n",
    "\n",
    "### Perceptron\n",
    "Neural network consists of few layers of perceptrons. Each perceptron simulates the operation of neuron. It collects input variables $x_i$, weights them with coefficients $w_i$, and sums the result to one value. The output value is obtained by scaling the sum between values 0...1 by using an activation function. For classification, the activation function is binary step function, and for regression, it is continuous, like sigmoid function. The neuron can be teached by updating the weights $w_i$.\n",
    "\n",
    "![Perceptron](kuvat/perceptron.svg)\n",
    "\n",
    "The output of the perceptron is\n",
    "\n",
    "$$\n",
    "  y = f\\left(\\mathbf{x} \\cdot \\mathbf{w} + w_0\\right) \n",
    "  = f\\left( \\Sigma_{i=1}^{n} (x_i w_i) + w_0 \\right)\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Activation functions\n",
    "\n",
    "Common activation functions, $f()$, are \n",
    " 1. Linear\n",
    " 1. Sigmoid\n",
    " 1. Hyperbolic tangent (tanh())\n",
    " 1. REctified Linear activation fUnction (RELU)\n",
    " \n",
    "Percepton networks using linear activation function are easy to train, but they cannot solve as complex problems as networks using non-linear activation functions. Sigmoid, also known as logistic function, was originally the default activation function, but it was replaced with hyperbolic tangent which seemed to be easier to train and performing better. \n",
    " \n",
    "A problem with both sigmoid and hyperbolic tangent is, however, that they saturate to constant output when the input is large or small, which leads to derivative approaching to zero which slows down the training.\n",
    "\n",
    "ReLu is simple activation function which supports fast learning due to being mostly linear and allows learning complex problems being piecewise non-linear. ReLu is especially usefull when training deep neural networks.\n",
    " \n",
    "Read more about activation function from [RELU for deep learning NN](https://machinelearningmastery.com/rectified-linear-activation-function-for-deep-learning-neural-networks/)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Sigmoid: $$ f(x)=\\frac{1}{1+e^{-x}} $$\n",
    "\n",
    "Tanh: $$f(x)=\\frac{e^x - e^{-x}}{e^x + e^{-x}} $$\n",
    "\n",
    "Relu: $$f(x) = \\begin{cases} 0 &  \\text{if} ~ x<0 \\\\Â x & \\text{otherwise} \\end{cases}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Implementation of activation functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-11-22T09:16:40.109105Z",
     "start_time": "2022-11-22T09:16:40.109087Z"
    },
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "sns.set()\n",
    "\n",
    "from ipywidgets import interact, interactive, fixed, interact_manual\n",
    "import ipywidgets as widgets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-11-22T09:16:40.110587Z",
     "start_time": "2022-11-22T09:16:40.110571Z"
    },
    "scrolled": false,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "sigmoid = lambda x: 1/(1+np.exp(-x))\n",
    "tanh = np.tanh\n",
    "relu = lambda x: [max(0,y) for y in x]\n",
    "x=np.linspace(-5,5)\n",
    "\n",
    "plt.plot(x, sigmoid(x), color='b', label='Sigmoid')\n",
    "plt.plot(x, tanh(x), color='g', label='Tanh')\n",
    "plt.plot(x, relu(x), color='r', label='ReLu')\n",
    "plt.axvline(0, color='black', linestyle='--')\n",
    "plt.legend()\n",
    "plt.axis([-5,5,-1.1,1.1])\n",
    "plt.savefig('activation_functions.svg')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Implementation of perceptron"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-11-22T09:16:40.112009Z",
     "start_time": "2022-11-22T09:16:40.111992Z"
    },
    "code_folding": [
     6
    ],
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "def perceptron(X,W,activation='relu'):\n",
    "    if activation == 'tanh':\n",
    "        f=np.tanh\n",
    "    elif activation == 'sigmoid':\n",
    "        f=lambda x: 1.0/(1+np.exp(-x))\n",
    "    else:\n",
    "        if activation != 'relu':\n",
    "            print(\"Unknown activation function, using ReLu\")\n",
    "        f=lambda x: max(0,x)\n",
    "    return f(W[0] + sum(X*W[1:]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Testing of a perceptron"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-11-22T09:16:40.113695Z",
     "start_time": "2022-11-22T09:16:40.113678Z"
    },
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "W=np.array([-1.0,1.0])\n",
    "N=100\n",
    "x=np.linspace(-6,6,N)\n",
    "y=np.zeros(N)\n",
    "\n",
    "def test(w0=0.0, w1=1.0, activation='sigmoid'):\n",
    "    global W,x\n",
    "    W[:] = (w0,w1)\n",
    "    y=[perceptron(xx, W, activation) for xx in x]\n",
    "    plt.figure()\n",
    "    plt.plot(x,y)\n",
    "    plt.axvline(0, linestyle='--')\n",
    "    \n",
    "interact(test, w0=(-3.0,3.0),w1=(-1.0,1.0), activation=['sigmoid', 'tanh', 'relu'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Learning algorithms\n",
    "\n",
    "When using Stochastic Gradient Descent (**SGD**) training,  the weights, $w_i$, are updated towards the gradient (multidimensional derivative) or the loss function. \n",
    "$$\n",
    "    w \\leftarrow w - \\eta \\left(\\alpha \\frac{\\partial R(w)}{\\partial w} + \\frac{\\partial L(w)}{\\partial w}\\right),\n",
    "$$\n",
    "\n",
    "where $\\eta$ is the learning rate, $\\alpha$ is the regularization term (L2 penalty for exessive model complexity), $R$ is a function related to model complexity and $L$ is a loss function. The weights of the model are simply updated to the direction where the model loss is reduced and model complexity is reduced. \n",
    "\n",
    "**Adams** is slightly more advanced and can optimize the parameters of the search, and find optimum faster. Another common learning method is **L-BFGS** (Limited memory, Broyden-Fletcher-Goldfarb-Shannon). It is also using the second derivatives of the search space and is thus faster, when the derivatives and memory resources are available. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Training of MLP](kuvat/mlp_training.svg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Multi layer perceptron\n",
    "\n",
    "A single perceptron can only handle simple problems. For more complex problems, a network of several layers of perceptrons are needed. These networks are called as Multi Layer Perceptron networks (MLP) or artificial neural networks (ANN). When the number of hidden layer is large, the network is called as Deep Neural Network (DNN) and it is one example of Deep Learning.\n",
    "\n",
    "![Perceptron](kuvat/mlp.svg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Training a neural network\n",
    "\n",
    "Training of a neural network is carried out through following steps\n",
    "\n",
    "1. The training data, including input data $X$ and the correct answers $y$ is selected\n",
    "1. The training data is split in one or more **batches**\n",
    "1. The training of the network is carried out it **iterations**, each iteration uses one batch of training data. The results of the network is compared against the correct output, and the coefficients of the network are updated to produce better results next time\n",
    "1. The training is proceed in next iteration, until all batches of input data is consumed\n",
    "1. At this time one **EPOCH** passed. The training often continues by using the same data again, and the whole training process can last from one EPOCH up to hundreds of EPOCHs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### When to stop training\n",
    "\n",
    " - The performance of the network usually improves when the traning continues.\n",
    " - The learning is fast in the beginning, but slows down after the network is well trained already \n",
    " - If the training continues too long, the network starts memorizing the training data and the performance is still seemingly improving, but the network's capability to predict new data starts decreasing. This situation is called as overfitting.\n",
    " - The amount of overfitting may be monitored by testing the prediction also in the separate validation set which is not used for training.\n",
    " - When the performance in the validation set starts decreasing, it is time to stop training.\n",
    "\n",
    "![When to stop trainig](kuvat/stoptraining.svg)\n",
    "\n",
    "Try to train multi layer neural network models in [Neural Network Playground](https://playground.tensorflow.org/).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Example\n",
    "\n",
    "The above Iris example can be classified using Multi Layer Perceptron Classifier (MLPC) but SVM already handled that problem well and because the number of samples in the dataset is only 150, it is only sufficient for training very thin MLPC. Therefore, lets create an artificial classification problem with 1000 samples and tree partly overlapping classes to make the problem more challengin and train an MLPC for solving it. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-11-22T09:16:40.115271Z",
     "start_time": "2022-11-22T09:16:40.115244Z"
    },
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn import datasets\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, r2_score\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from snippets import plotDB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-11-22T09:16:40.119485Z",
     "start_time": "2022-11-22T09:16:40.119453Z"
    },
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "# Make a random 3-class classification problem, with 1000 samples and 2 features\n",
    "X,y=data=datasets.make_classification(n_samples=1000, n_features=2, n_classes=3, random_state=2,\n",
    "                                  n_clusters_per_class=1, n_informative=2, n_redundant=0)\n",
    "\n",
    "# Create a multilayer perceptron classifier with 10 and 6 perceptrons in hidden layer\n",
    "predictor = MLPClassifier(hidden_layer_sizes=(10,6), max_iter=1000)\n",
    "predictor.fit(X=X, y=y)\n",
    "classes = predictor.predict(X)\n",
    "\n",
    "# Assess the accuracy of the classifier\n",
    "accuracy = accuracy_score(y_true=y, y_pred=classes)\n",
    "M=confusion_matrix(y, classes)\n",
    "\n",
    "# Plot the results and decision boundaries\n",
    "print(M)\n",
    "print(\"Prediction accuracy is\", accuracy)\n",
    "plotDB(predictor, X=X, y=y, figsize=(5,5))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Digits dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-11-22T09:16:40.121179Z",
     "start_time": "2022-11-22T09:16:40.121161Z"
    },
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "digits = datasets.load_digits()\n",
    "X=digits.data\n",
    "y=digits.target\n",
    "\n",
    "predictor = MLPClassifier(hidden_layer_sizes=(6,), max_iter=1000)\n",
    "predictor.fit(X=X, y=y)\n",
    "\n",
    "classes = predictor.predict(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-11-22T09:16:40.122604Z",
     "start_time": "2022-11-22T09:16:40.122586Z"
    },
    "scrolled": true,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "print(\"Prediction accuracy is\", accuracy)\n",
    "\n",
    "# Assess the accuracy of the classifier\n",
    "accuracy = accuracy_score(y_true=y, y_pred=classes)\n",
    "cvscore = cross_val_score(predictor, X, y).mean()\n",
    "M=confusion_matrix(y, classes)\n",
    "\n",
    "# Plot the results and decision boundaries\n",
    "print(M)\n",
    "print(\"Prediction accuracy is   \", accuracy)\n",
    "print(\"Prediction accuracy in CV\", cvscore)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Time series analysis\n",
    "\n",
    "- In it's standard form, the neural network has no understanding of time, and it is therefore not very suitable for time series analysis, for example in predicting future values\n",
    "- Several variations of Neural Networks are however suitable for time series analysis\n",
    "- Recurrent Neural Networks (RNN), for example Recurreng Gate Units (GRU) and Long-Short-Term Memory (LSTM) networks are common methods for time series analysis with ANN\n",
    "- Read more from [Recurrent Neural Networks cheatsheet](https://stanford.edu/~shervine/teaching/cs-230/cheatsheet-recurrent-neural-networks)\n",
    "\n",
    "![RNNimage](kuvat/RNN.svg)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Convolutional neural network\n",
    "\n",
    "- In deep image analysis networks, it is efficient to combine neural network with efficient image processing functions, such as convolution and subsampling (pooling).\n",
    "- The following CNN network consists of \n",
    "  - 4 filters in the first convolutional layer , producing four feature maps for each image channel\n",
    "  - a subsampling layer, for example 3x3 max pooling with stride=3, to reduce the image size to one third\n",
    "  - 3 filters in the second convolutional layer, producing three features maps from each previous feature maps\n",
    "  - a second subsampling layer, to reduce the image size\n",
    "  - A fully connected layer of neurons\n",
    "  - An output layer of output neurons\n",
    "\n",
    "![Convolutional neural network](kuvat/cnn.png)\n",
    "\n",
    "A nice description of convolutional neural network (CNN) is [here](https://wiki.pathmind.com/convolutional-network)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Keras and Pytorch\n",
    "\n",
    "MLP classifier and regressors use only CPU resources, but to utilize real power of ANN, they are often ran in massive parallel hardware, such as GPU:s. This is not necessary for simple neural network models shown above, but they become more important when the number of hidden layers in the network model increases and the model becomes deeper. \n",
    "\n",
    "Frameworks often used for Deep Neural Networks are for example Keras, Tensorflow and PyTorch.\n",
    "\n",
    " - Keras is a high level libary which uses underlying Tensorflow\n",
    " - PyTorch is lower level python interface for Torch library\n",
    "\n",
    "[Keras examples](https://keras.io/examples/)\n",
    "\n",
    "[Keras vs Pytorch for Deep Learning](https://towardsdatascience.com/keras-vs-pytorch-for-deep-learning-a013cb63870d)\n",
    "\n",
    "[PyTorch vs Keras](https://towardsdatascience.com/keras-vs-pytorch-for-deep-learning-a013cb63870d)\n",
    "\n",
    "[Why to choose Keras](https://keras.io/why_keras/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Example code for simple Keras processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-11-22T09:16:40.097273Z",
     "start_time": "2022-11-22T09:16:39.841033Z"
    },
    "scrolled": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'tensorflow'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_3893910/2984475335.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mmatplotlib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpyplot\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mtensorflow\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mkeras\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeras\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mlayers\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'tensorflow'"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras.layers import CenterCrop\n",
    "from tensorflow.keras.layers import Rescaling\n",
    "\n",
    "# Example image data, with values in the [0, 255] range\n",
    "training_data = np.random.randint(0, 256, size=(64, 200, 200, 3)).astype(\"float32\")\n",
    "\n",
    "# Some preprocessing of the data\n",
    "cropper = CenterCrop(height=150, width=150)\n",
    "scaler = Rescaling(scale=1.0 / 255)\n",
    "\n",
    "output_data = scaler(cropper(training_data))\n",
    "print(\"shape:\", output_data.shape)\n",
    "print(\"min:\", np.min(output_data))\n",
    "print(\"max:\", np.max(output_data))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Convolutional neural network with Keral\n",
    "\n",
    "The CNN model consists of layers, and the Keras API allows building the layered CNN model in very straightforward manner. Keras contains also many functions for preprocessing images and generating variations from existing image database, because the model contains plenty of parameters and needs therefore a large set of training images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-11-22T09:16:40.099941Z",
     "start_time": "2022-11-22T09:16:40.099920Z"
    },
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "# Define an input layer, which can be arbitrary size, but include 3 channels (RGB)\n",
    "inputs = keras.Input(shape=(None, None, 3))\n",
    "\n",
    "# Center-crop images to 150x150\n",
    "x = CenterCrop(height=150, width=150)(inputs)\n",
    "\n",
    "# Rescale images to [0, 1]\n",
    "x = Rescaling(scale=1.0 / 255)(x)\n",
    "\n",
    "# Apply some convolution and pooling layers\n",
    "x = layers.Conv2D(filters=32, kernel_size=(3, 3), activation=\"relu\")(x)\n",
    "x = layers.MaxPooling2D(pool_size=(3, 3))(x) # 50x50\n",
    "x = layers.Conv2D(filters=32, kernel_size=(3, 3), activation=\"relu\")(x)\n",
    "x = layers.MaxPooling2D(pool_size=(3, 3))(x) # 16x16\n",
    "x = layers.Conv2D(filters=32, kernel_size=(3, 3), activation=\"relu\")(x)\n",
    "\n",
    "# Apply global average pooling to get flat feature vectors\n",
    "x = layers.GlobalAveragePooling2D()(x)\n",
    "\n",
    "# Add a dense classifier on top\n",
    "num_classes = 10\n",
    "outputs = layers.Dense(num_classes, activation=\"softmax\")(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-11-22T09:16:40.101638Z",
     "start_time": "2022-11-22T09:16:40.101621Z"
    },
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "# Instantiate the model object\n",
    "model = keras.Model(inputs=inputs, outputs=outputs)\n",
    "\n",
    "data = np.random.randint(0, 256, size=(64, 200, 200, 3)).astype(\"float32\")\n",
    "processed_data = model(data)\n",
    "print(processed_data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-11-22T09:16:40.103617Z",
     "start_time": "2022-11-22T09:16:40.103596Z"
    },
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "model.summary()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "The next steps would be the compile and train the model. Then the performance in the training set and validation set would be tested. You can see the full process in [OpenCV and Keras | Traffic Sign Classification for Self-Driving Car](https://www.geeksforgeeks.org/opencv-and-keras-traffic-sign-classification-for-self-driving-car/?ref=rp)\n",
    "\n",
    "Notice that the model consist of nearly 20 thousand parameters. I takes plenty of data and time to train the model. The training time can be from 10 minutes to days, depending of the complexity of the model and the calculation capacity available."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Pre trained models\n",
    "\n",
    "Since the training of the neural network models is so time consuming, an important topics in deep learning are pre trained models and transfer learning (the model trained to one application is used in another related application without retraining, or partial re-training.\n",
    "\n",
    "Some huge deep networks can be also used as general purpose neural networks. One of the biggest network this far is the Generative Pre-trained Transformer 3 ([GPT-3](https://en.wikipedia.org/wiki/GPT-3)), which is suitable for many Natural Language Processing (NLP) tasks. GPT-3 sisÃ¤ltÃ¤Ã¤ 175 billion ($175 \\cdot 10^9$) parameters. GPT-3 can generate text wich is difficult to distinguish from a human writer.\n",
    "\n",
    "[VGG16](https://www.geeksforgeeks.org/vgg-16-cnn-model/) is a winning model from Visual Geometry Group Lab of Oxford University, presented in 2014 in the paper *Very deep convolutional networks fo large-scale image recognition*.\n",
    "\n",
    "[A robot wrote this entire article. Are you scared yet, human](https://www.theguardian.com/commentisfree/2020/sep/08/robot-wrote-this-article-gpt-3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## More information\n",
    "- [OpenCV and Keras | Traffic Sign Classification for Self-Driving Car](https://www.geeksforgeeks.org/opencv-and-keras-traffic-sign-classification-for-self-driving-car/?ref=rp)\n",
    "- Read more from [Keras for engineers](https://keras.io/getting_started/intro_to_keras_for_engineers/)\n",
    "- [Convolutional Neural Networks cheatsheet](https://stanford.edu/~shervine/teaching/cs-230/cheatsheet-convolutional-neural-networks)\n",
    "- [Recurrent Neural Networks cheatsheet](https://stanford.edu/~shervine/teaching/cs-230/cheatsheet-recurrent-neural-networks)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Summary\n",
    "\n",
    "- Artificial Neural Networks (ANN) based on perceptrons, are versatile machine learning methods for both regression and classification. Deep and dense networks can learn to handle complex tasks, but the deeper the network, the more training data is needed.\n",
    "- Deep learning can be implemented using many hidden layers in ANN. Deep learning requires large amount of training data.\n",
    "- Deep learning is often used for image processing with convolutional neural networks\n",
    "- Keras and PyTorch are common libraries for implementing deep learning. They can utilize both CPU:s and NVidia GPU:s for training and predicting"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
