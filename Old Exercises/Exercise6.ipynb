{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Applied Machine Learning (2020), exercises\n",
    "\n",
    "\n",
    "## General instructions for all exercises\n",
    "\n",
    "Before you turn this problem in, make sure everything runs as expected. First, **restart the kernel** (in the menubar, select Kernel$\\rightarrow$Restart) and then **run all cells** (in the menubar, select Cell$\\rightarrow$Run All).\n",
    "\n",
    "Follow the instructions and fill in your solution under the line marked by tag\n",
    "\n",
    "> YOUR CODE HERE\n",
    "  \n",
    "Having written the answer, execute the code cell by and pressing `Shift-Enter` key combination. The code is run, and it may print some information under the code cell. The focus automatically moves to the next cell and you may \"execute\" that cell by pressing `Shift-Enter` again, until you have reached the code cell which tests your solution. Execute that and follow the feedback. Usually it either says that the solution seems acceptable, or reports some errors. You can go back to your solution, modify it and repeat everything until you are satisfied. Then proceed to the next task.\n",
    "   \n",
    "Repeat the process for all tasks.\n",
    "\n",
    "The notebook may also contain manually graded answers. Write your manualle graded answer under the line marked by tag:\n",
    "\n",
    "> YOUR ANSWER HERE\n",
    "\n",
    "Manually graded tasks may be text, pseudocode, or mathematical formulas. You can write formulas with $\\LaTeX$-syntax by enclosing the formula with dollar signs (`$`), for example `$f(x)=2 \\pi / \\alpha$`, will produce $f(x)=2 \\pi / \\alpha$\n",
    "\n",
    "When you have passed the tests in the notebook, and you are ready to submit your solutions, download the whole notebook, using menu `File -> Download as -> Notebook (.ipynb)`. Save the file in your hard disk, and submit it in [Moodle](https://moodle.uwasa.fi) under the corresponding excercise.\n",
    "\n",
    "Your solution should be an executable Python code. Use the code already existing as an example of Python programing and read more from the numerous Python programming material from the Internet if necessary. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "NAME = \"\"\n",
    "Student_number = \"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ICAT3190, Module 5, Excercise 6\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sound sample classification\n",
    "\n",
    "The following sound sample contains wind turbine noise, tractor noise and bird noise. The samples are separated so that only one noise source is dominating at the time. Your task is to make a classifier algorith which can separate the noise sources from each other.\n",
    "\n",
    "These recording are made in Honkajoki, near Kirkkokallio wind park during [WindSoMe](https://osuva.uwasa.fi/handle/10024/11290) project.\n",
    "\n",
    "You can listen the sound sample using the following link."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import Audio\n",
    "audio = Audio(filename='data/sample.wav')\n",
    "display(audio)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature extraction\n",
    "\n",
    "The first step in classification is to generate features. In this case, the whole sound signal of 180 seconds is divided in 360 pieces, and many audio features are generated for each sample. Since we did not know which audio features might be most important, we created (almost) all that we knew, and let a machine learning algorithm to choose the best features. The calculate the following features:\n",
    "\n",
    "| Feature # | Explanation \n",
    "| ----- | :---- |\n",
    "| 0-128 | 128 Mel Spectral coefficients (these describe how the sound energy is distributed in psychoachoustically relevant spectrum) |\n",
    "| 128-168 | 40 Mel Cepstral coefficiens |\n",
    "| 168-175 | 7 Spectral contrast coefficients |\n",
    "| 175-178 | Three polynomial coefficients (when fitting a third order polynomial to the data) |\n",
    "| 178:181 |  Three LPC filter model coefficients |\n",
    "| 181 |  RMS value of the signal  |\n",
    "| 182 | Zero crossing rate |\n",
    "| 183 | Spectral centroid |\n",
    "| 184 | Spectral bandwidth |\n",
    "| 185 | Spectral flatness |\n",
    "| 186 | True sound class |\n",
    "\n",
    "The sound classes are\n",
    " 1. Wind turbine noise\n",
    " 2. Bird calls\n",
    " 3. Tractor noise\n",
    " 4. Rain\n",
    " 5. Wind\n",
    "\n",
    " The most of the features are calculated with [LibRosa](https://librosa.org/).\n",
    " \n",
    " The pre-calculated features are available in the data file, and you can read it with the following code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "Data=pd.read_hdf('data/Features.hdf')\n",
    "print(Data.shape)\n",
    "Data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 1: Visualize the data using PCA\n",
    "\n",
    "It is often a good idea to first visualize the data using PCA coordinates, to see if the classes are nicely separated or not. It helps deciding how advanced classifier is needed. \n",
    "\n",
    "Scale the data, calculate PCA and plot in two first principal components, coloring the samples using class labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "62ed59899beb0b8cb4ef4056617af18e",
     "grade": false,
     "grade_id": "cell-6bdba115ad5623d6",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Store your principal component as pc\n",
    "#pc= \n",
    "\n",
    "# YOUR CODE HERE\n",
    "raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "63ece099bc6a7bd1ace0a963061145c4",
     "grade": true,
     "grade_id": "cell-b6e25bd66ef1bc9c",
     "locked": true,
     "points": 3,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "assert('pc' in locals()), \"No pc variable found! Did you forgot to store your principal components as pc?\"\n",
    "\n",
    "if abs(pc.max())>50:\n",
    "    print(\"You apparently forgot to standardize the variables before PCA?\")\n",
    "    assert(False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 2\n",
    "\n",
    "\n",
    "### Classification using extrarees classifier\n",
    "\n",
    "\n",
    "Train an extratrees classifier to classify the samples directly in the feature space. Do not use PCA as a preprocessor. Use 25% of the samples in the training set. Report the accuracy in training set, with cross validation (5 folds) and in the test set.\n",
    "\n",
    "This problem is not really difficult, so try to use shallow decision trees, no deeper than 3. \n",
    "\n",
    "Print the accuracy score in training set, crossvalidation and test set. Calculate also the confusion matrix as variable `M` and print it.\n",
    "\n",
    "Print also the `.feature_importances_` variable of the predictor, for example using the `plt.stem()` function. Read from the documentation what does it mean and how could this information could be used.\n",
    "\n",
    "You can try to adjust at least the following parameters of the classifier `n_estimators`, `max_depth`, `min_samples_leaf`. Try to let your classifier only as much degrees of freedom as necessary for gaining good classification results, but not any more to avoid too complex decision boundaries and possible problems in generalization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "d1f7d1427c7f764601ca5659c73b6930",
     "grade": false,
     "grade_id": "cell-1e7f4987873667e9",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.ensemble import ExtraTreesClassifier\n",
    "\n",
    "\n",
    "train_score=\n",
    "test_score=\n",
    "cv_score=\n",
    "M=\n",
    "\n",
    "# YOUR CODE HERE\n",
    "raise NotImplementedError()\n",
    "plt.figure(figsize=(10,3))\n",
    "plt.stem(predictor.feature_importances_)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "e1876a6c82e8cf12748f062251549e22",
     "grade": true,
     "grade_id": "cell-b72aebbbc43db552",
     "locked": true,
     "points": 3,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "import sklearn.ensemble\n",
    "print(predictor)\n",
    "assert(type(predictor)==sklearn.ensemble.ExtraTreesClassifier), \"The classifier is of wrong type\"\n",
    "assert(test_score>0.9), \"The predictor cannot classify good enough\"\n",
    "assert(np.diag(M).sum()/M.sum()>0.9), \"Confusion matrix does not look good\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 3\n",
    "\n",
    "### Classification using gradient boosted tree classifier\n",
    "\n",
    "Train an GradientBoostingClassifier to classify the samples directly in the feature space. Do not use PCA as a preprocessor. Use 25% of the samples in the training set. Report the accuracy in training set, with cross validation (5 folds) and in the test set. Calculate also the confusion matrix as variable M and print it.\n",
    "\n",
    "Print also the .feature_importances_ variable of the predictor, for example using the plt.stem() function. Read from the documentation what does it mean and how could this information could be used.\n",
    "\n",
    "You can try to adjust at least the following parameters of the classifier `n_estimators`, `max_depth`, `min_samples_leaf` and `leraning_rate`. Try to let your calssifier only as much degrees of freedom as necessary for gaining good classification results, but not any more to avoid too complex decision boundaries and possible problems in generalization.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "71e7ca61a939204222cec2370e128326",
     "grade": false,
     "grade_id": "cell-08f233ff584f1e81",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "\n",
    "\n",
    "#train_score=\n",
    "#test_score=\n",
    "#cv_score=\n",
    "#M=\n",
    "\n",
    "# YOUR CODE HERE\n",
    "raise NotImplementedError()\n",
    "plt.figure(figsize=(10,3))\n",
    "plt.stem(predictor.feature_importances_)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "cad89f593c4cc28d44a57e7dd03778a9",
     "grade": true,
     "grade_id": "cell-5d54747350d8fd32",
     "locked": true,
     "points": 3,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "import sklearn.ensemble\n",
    "print(predictor)\n",
    "assert(type(predictor)==sklearn.ensemble.GradientBoostingClassifier), \"The classifier is of wrong type\"\n",
    "assert(test_score>0.9), \"The predictor cannot classify good enough\"\n",
    "assert(np.diag(M).sum()/M.sum()>0.9), \"Confusion matrix does not look good\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
